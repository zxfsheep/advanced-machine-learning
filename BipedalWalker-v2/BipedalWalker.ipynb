{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an Actor-Critic style method on the `BipedalWalker-v2` task from the [*OpenAI gym* toolkit](https://gym.openai.com/envs/BipedalWalker-v2/), using basic TensorFlow. I improvised here and there for the specific problem. In particular, we use a **rollout estimator with a random horizon** to approximate TD($\\lambda$) with a very simple and cheap implementation.\n",
    "\n",
    "The task is to help a two-legged robot learn how to walk forward. Reward is given for moving forward, total 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points. It poses a combination of challenges: the robot not only needs to learn how to walk, but also to walk fast without falling, with as little action as possible, on a randomly generated terrain, and with continuous action parameters.\n",
    "\n",
    "Here is the video for my final walker that solved the task:\n",
    "[![Video of final Walker for BipedalWalker-v2](http://img.youtube.com/vi/WZ2lq4uzs1k/0.jpg)](http://www.youtube.com/watch?v=WZ2lq4uzs1k \"Final walker for BipedalWalker-v2\")\n",
    "\n",
    "\n",
    "The state and action parameters can be found at [here](https://github.com/openai/gym/wiki/BipedalWalker-v2). Most of them are self-explanatory. The \"10 lidar readings\" refer to the terrain measurements made by that moving red laser. It actually updates every step, even though the red laser seems to scan only once in a while.\n",
    "\n",
    "When terrain is more rugged, the partial information from lidar readings is not sufficient to learn a maneuver, and we need to use **LSTM** to provide historical data. For this task, since the terrain, despite being random, is relatively flat, we will only use the current data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Installation of the box2d environment on windows(this took me a long time to figure out; unfortunately WSL does not support GPU):\n",
    "* pybox2d has to be built from source to be compatible with Tensorflow\n",
    "* To do that, first download `swig` compiled for windows, and add the directory to system `PATH`.\n",
    "* Install Microsoft Visual C++ 14.0 and add the following specific folders to system `PATH`(maybe also `LIB` or `INCLUDE`):\n",
    "\n",
    "`C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64`\n",
    "\n",
    "`C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\lib\\amd64`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many variations of the Actor-Critic model, in terms of both implementation as well as theoretical improvements. The specific implementation here is following my own preference and understanding. From my point of view, AC can be seen as a combination of policy gradient and TD or Q-learning. The actor network, which learns to choose the best action, tends to reduce bias but increase variance in the performance. The critic network, on the other hand, tends to decrease variance but bring bias by tring to learn the inherent long term value of a state-action pair. The fact that the two networks can get trained together is the most fascinating aspect for me.\n",
    "\n",
    "One difficulty dealing with continuous action AC is that, since the actor merely outputs an action, it is not clear what the action distribution is. To train the actor properly by backprop, I decided to stick to that action, which makes the actor being trained deterministic. To encourage exploration, we will have to introduce randomness somewhere.\n",
    "\n",
    "Therefore we will keep a separate copy of the actor, updated by the current actor once in a while. We shall call this actor the **explorer**. We generate a memory buffer using the explorer mixed with certain randomness. The randomness can be tuned down gradually to decrease exploration as time goes on. We train the actor and critic over minibatches from the replay memory. \n",
    "\n",
    "Another difficulty for this specific problem is that the environment does not support online rollout from a specific memory snapshot. It is technically not allowed to implement it, because the terrain for each replay is randomized, and in order to do an online rollout, we will have to know the specific position given the random seed of the terrain. But the problem is set up so that we should not use position coordinates. This limitation prohibits us from using some techniques based on online rollouts.\n",
    "\n",
    "To reduce bias, we shall use a novel **rollout estimator with a random horizon** to train the critic network, where the minimum horizon length also decreases as training goes on. This is inspired by TD($\\lambda$) learning and should achieve similar effect. But this implementation is much easier and cheaper compared to saving a trace. Experimentally, starting with longer horizons avoids early convergence to a bad local minimum. The intuitions are the following. Initially the movement is rather random and nonoptimal, so the reward over a short horizon is not that informative, and taking reward over a longer horizon smooths out the randomness. Also, a short horizon will force the critic network to converge locally, as the state does not change much over a short horizon. Over a longer horizon, the initial and final states and actions are rather different, so the critic network is converging in a more global fashion. However, at later stage of the training, we would like both networks to converge globally as well as locally, so we tune down the minimum horizon. Hopefully by that time, the network is already near a global optimum, and the randomness is also low, so even short time reward is close to an optimal action.\n",
    "\n",
    "Furthermore, to avoid unstablity, we also keep a separate copy of the critic for the value estimator, and **partially update** it whenever we update the explorer. \n",
    "\n",
    "The hyperparameters here are definitely not optimal, as I am training on my laptop, which takes a few days. To be safe, I chose very conservative parameters(for example, the randomness is dropping very slowly, and training is done quite infrequently compared to the total trial episodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let us import common packages and the `BipedalWalker-v2` environment.\n",
    "\n",
    "Note that in Windows system, even if we render with `rgb_array` mode, it still tries to output the picture in a separate window. On Linux, one can run `Xvfb` to avoid it, but unfortunately it seems unavailable for Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFUVJREFUeJzt3X+sZOV93/H3J4DBNW4WMLb2l4tdbxOTqCxwi7FoKgfsBEhaiBS3kCpGKdJuVSzZstUGUqm11SIlamNaqyllHRzjyDWm2C4UQRyKsRJLNXjXxngxJqxtFC53y9pgbNMoxJBv/7jPhWF37r1z78zszJz7fkmjOec5zznzPDtzP3P2mefMpKqQJHXPT0y6AZKk8TDgJamjDHhJ6igDXpI6yoCXpI4y4CWpo8YW8EkuTPJIkgNJrh7X40iS+ss45sEnOQb4M+AdwDzwZeDyqvrGyB9MktTXuM7gzwEOVNW3q+qvgJuBS8b0WJKkPo4d03G3Ao/3rM8Db1mu8sknv6a2bz9tTE2RXnLccZNuwcby4x8Pt//RfL6GbeuoPf74Yzz99PcyzDHGFfD9GvWysaAku4BdAFu3vp677to7pqZoo9qyZdIt0JKFhcHqTeNzNmjbR+2ii+aGPsa4An4e2N6zvg142T9TVe0B9gCcccacX4ijoUxjMOglS89Pv7Cc9uduufZNKvjXYlwB/2VgR5I3AE8AlwG/NqbH0gYz7YGg5XXpuZuF4B9LwFfV80neDXwOOAb4aFU9NI7HUrd1KRC0MUxT8I/rDJ6quhO4c1zHV7cY5Oq6SQT/2AJeWo2hLr3872DUYW/A66gx0KWV9f6NjGKKqAGvkTPIpelgwGsohrk0vQx4Dcwwl2aLAa8VGerS7PL74LWiabpoQ9LaGPBalSEvzSYDXgMx5KXZY8BrYIa8NFsMeK2JIS/NDgNea2bIS7PBgNe6GPLS9DPgtW6GvDTdDHgNxZCXppcBr6EZ8tJ0MuA1Eoa8NH0MeI3MwoJBL02Tob5sLMljwI+AF4Dnq2ouycnAp4DTgMeAf1xV3x+umZKktRrFGfzPV9XOqppr61cD91TVDuCetq4NxLN4aTqMY4jmEuCmtnwTcOkYHkNTzuEaafKGDfgC/jjJviS7WtnrquogQLt/7ZCPoRlmyEuTM+wPfpxXVQtJXgvcneSbg+7Y3hB2AWzd+vohm6FptrDgD4dIkzDUGXxVLbT7Q8BngXOAJ5NsBmj3h5bZd09VzVXV3CmnnDpMMzQDPJOXjr51B3ySVyV59dIy8AvAfuB24IpW7QrgtmEbqW4w5KWja5ghmtcBn02ydJz/XlV/lOTLwC1JrgT+HHjn8M1UVzhcIx096w74qvo2cEaf8qeAC4ZplLrNkJeODq9k1UQ4XCONnwGviXGuvDReBrwmzpCXxsOAl6SOMuA1FTyLl0bPgNfUMOSl0TLgNVUMeWl0DHhNHWfXSKNhwGtqGfLScAx4TTVDXlo/A15Tz5CX1seA10ww5KW1M+A1Mwx5aW0MeM0UQ14anAGvmWPIS4Mx4DWTDHlpdQa8ZpYhL63MgNdMM+Sl5Rnwmnl+tYHU36oBn+SjSQ4l2d9TdnKSu5M82u5PauVJ8uEkB5I8mOSscTZe6mXISy83yBn8x4ALDyu7GrinqnYA97R1gIuAHe22C7h+NM2UBmPISy9ZNeCr6k+Apw8rvgS4qS3fBFzaU/7xWvQlYFOSzaNqrDQIQ15atN4x+NdV1UGAdv/aVr4VeLyn3nwrO0KSXUn2Jtn71FPfXWczJEnLGfWHrOlTVv0qVtWeqpqrqrlTTjl1xM3QRudZvLT+gH9yaeil3R9q5fPA9p562wD/1DQRzq7RRrfegL8duKItXwHc1lP+rjab5lzgB0tDOdKkGPLaqAaZJvlJ4P8AP5VkPsmVwG8D70jyKPCOtg5wJ/Bt4ADwEeBfjKXV0hoZ8tqIjl2tQlVdvsymC/rULeCqYRsljcPCAmzZMulWSEePV7JqQ/FMXhuJAa8Nx5DXRrHqEI3URQ7XdNNKb94b8fk24LVhGfKzZ5j/fS23b5dfAwa8NrSlP/ou/5HPkkkMn3U5+A14Cc/mj6ZZ+QykC8FvwEsaq1kJ9EEd3p9pDnwDXmo8i1+frgX4Wk3zmb4BL/VwTL6/jR7i6zFM8C8swI9/PHwbDHipj412Nm+AHz2rBf8onwsDXlpGl0LeAJ9+43iODHhpBbMU8oa4DmfASzPKQNdqDHhpFZP64NUA17AMeGlA4xiuMcQ1Tga8tAZrDXkDXJNkwEtrdHjIG+KaVga8tA6GumbBIL/J+tEkh5Ls7yn7QJInkjzQbhf3bLsmyYEkjyT5xXE1XJK0skF+0eljwIV9yq+rqp3tdidAktOBy4Cfafv81yTHjKqxkqTBrRrwVfUnwNMDHu8S4Oaqeq6qvgMcAM4Zon2SpHUa5jdZ353kwTaEc1Ir2wo83lNnvpUdIcmuJHuT7H3qqe8O0QxJUj/rDfjrgb8N7AQOAr/bytOnbvU7QFXtqaq5qpo75ZRT19kMSdJy1hXwVfVkVb1QVX8NfISXhmHmge09VbcBzjeQpAlYV8An2dyz+ivA0gyb24HLkhyf5A3ADuD+4ZooSVqPVefBJ/kk8DbgNUnmgX8LvC3JThaHXx4DdgNU1UNJbgG+ATwPXFVVL4yn6ZKklawa8FV1eZ/iG1eofy1w7TCNkiQNb5hZNJKkKWbAS1JHGfCS1FEGvCR1lAEvSR1lwEtSRxnwktRRBrwkdZQBL0kdZcBLUkcZ8JLUUQa8JHWUAS9JHWXAS1JHGfCS1FEGvCR1lAEvSR1lwEtSR60a8Em2J7k3ycNJHkrynlZ+cpK7kzza7k9q5Uny4SQHkjyY5Kxxd0KSdKRBzuCfB95fVW8GzgWuSnI6cDVwT1XtAO5p6wAXATvabRdw/chbLUla1aoBX1UHq+orbflHwMPAVuAS4KZW7Sbg0rZ8CfDxWvQlYFOSzSNvuSRpRWsag09yGnAmcB/wuqo6CItvAsBrW7WtwOM9u823ssOPtSvJ3iR7n3rqu2tvuSRpRQMHfJITgU8D762qH65UtU9ZHVFQtaeq5qpq7pRTTh20GZKkAQ0U8EmOYzHcP1FVn2nFTy4NvbT7Q618Htjes/s2YGE0zZUkDWqQWTQBbgQerqoP9Wy6HbiiLV8B3NZT/q42m+Zc4AdLQzmSpKPn2AHqnAf8OvD1JA+0st8Cfhu4JcmVwJ8D72zb7gQuBg4AfwH8xkhbLEkayKoBX1VfpP+4OsAFfeoXcNWQ7ZIkDckrWSWpowx4SeooA16SOsqAl6SOGmQWjUZo9+4PDFTvhhsGqydJyzHgR2TQ4D57y641H8+wl7QeBvwIDRreaznWvoU9hr2kdTHgp9zhbxqGvaRBGfAzpjfwDx8WMvAl9TLgZ9hyZ/cGvSRwmmSnjPIzAEmzz4CXpI4y4CWpowx4SeooP2QdoX0LeybdBEl6kQE/IuudubJ7bo6zt5z94vq+hX3csHfviFolaSNziEaSOsqAnzJnbzmb3XNzk26GpA4Y5Ee3tye5N8nDSR5K8p5W/oEkTyR5oN0u7tnnmiQHkjyS5BfH2QFJUn+DjME/D7y/qr6S5NXAviR3t23XVdV/7K2c5HTgMuBngC3A/07yd6rqhVE2XJK0slXP4KvqYFV9pS3/CHgY2LrCLpcAN1fVc1X1HeAAcM4oGitJGtyaxuCTnAacCdzXit6d5MEkH01yUivbCjzes9s8K78hrMnWrTniJkk60sDTJJOcCHwaeG9V/TDJ9cC/A6rd/y7wz4B+iVt9jrcL2AWwdevr19zwJ16aWbhsyD/xxBEPK0kbxkABn+Q4FsP9E1X1GYCqerJn+0eAO9rqPLC9Z/dtwMLhx6yqPcAegDPOmBsqiXvDvtfhwW/gS9pIBplFE+BG4OGq+lBP+eaear8C7G/LtwOXJTk+yRuAHcD9o2tyt5x99tnsW9g36WZI6qBBzuDPA34d+HqSB1rZbwGXJ9nJ4vDLY8BugKp6KMktwDdYnIFz1bhn0GxdJh89Y5e0ka0a8FX1RfqPq9+5wj7XAtcO0a5V9Ya6QS5JR5rZ76Ix1CVpZX5VgSR11MyewXeNH7RKGjXP4KfE2VvOfvFrg/3CMUmjYMBLUkcZ8FPI4RpJo+AY/BTyF50kjYJn8JLUUQb8hO3bt+9lv8m6ZIvfkilpSAa8JHWUAS9JHWXAS1JHGfBTpHd65ILftSNpSAa8JHWUAS9JHWXAS1JHGfBTzLnwkoZhwE/Qnt27J90ESR02yI9un5Dk/iRfS/JQkg+28jckuS/Jo0k+leQVrfz4tn6gbT9tvF2QJPUzyJeNPQecX1XPJjkO+GKSu4D3AddV1c1J/htwJXB9u/9+Vb0pyWXA7wD/ZEzt7zSnSmq9dh/8h0cWHvmNGC+30peYDrPvavsPs+8A+9+w+X+tcoDuGuRHtwt4tq0e124FnA/8Wiu/CfgAiwF/SVsGuBX4L0nSjqM+eue/+02Sk7F7y2GBOK6wO0pBuaXfcYb5Fuphv8F6go+9e1efN7tBjj3AczXtbx4ZJHeTHMPiP8WbgN8D/gPwpap6U9u+Hbirqn42yX7gwqqab9u+Bbylqr633PE3nb6pfu4Pf+6lglk9W5iBP/yRP3ZH+rxlzyr1pT4WVnoNDvn6/NO5P+WZvc8MNdNioO+Dr6oXgJ1JNgGfBd7cr1q779egI95FkuwCdgGcePIrX37GMatnCxvxDGkj9llq+v5PacmQr8/jvjvE/s2aZtFU1TPAF4BzgU1Jlt4gtgELbXke2A7Qtv8k8HSfY+2pqrmqmjvhxFesr/WSpGUNMovm1HbmTpJXAm8HHgbuBX61VbsCuK0t397Wads/7/i7JB19gwzRbAZuauPwPwHcUlV3JPkGcHOSfw98Fbix1b8R+MMkB1g8c79sDO2WJK1ikFk0DwJn9in/NnBOn/K/BN45ktZJktbNK1klqaMMeEnqKANekjrKgJekjjLgJamjDHhJ6igDXpI6yoCXpI4y4CWpowx4SeooA16SOsqAl6SOMuAlqaMMeEnqKANekjrKgJekjjLgJamjDHhJ6qhBfnT7hCT3J/lakoeSfLCVfyzJd5I80G47W3mSfDjJgSQPJjlr3J2QJB1pkB/dfg44v6qeTXIc8MUkd7Vt/7Kqbj2s/kXAjnZ7C3B9u5ckHUWrnsHXomfb6nHtVivscgnw8bbfl4BNSTYP31RJ0loMNAaf5JgkDwCHgLur6r626do2DHNdkuNb2Vbg8Z7d51uZJOkoGijgq+qFqtoJbAPOSfKzwDXATwN/DzgZ+M1WPf0OcXhBkl1J9ibZ+5fP/tW6Gi9JWt6aZtFU1TPAF4ALq+pgG4Z5DvgD4JxWbR7Y3rPbNmChz7H2VNVcVc2dcOIr1tV4SdLyBplFc2qSTW35lcDbgW8ujasnCXApsL/tcjvwrjab5lzgB1V1cCytlyQta5BZNJuBm5Icw+Ibwi1VdUeSzyc5lcUhmQeAf97q3wlcDBwA/gL4jdE3W5K0mlUDvqoeBM7sU37+MvULuGr4pkmShuGVrJLUUQa8JHWUAS9JHWXAS1JHGfCS1FEGvCR1lAEvSR1lwEtSRxnwktRRBrwkdZQBL0kdZcBLUkcZ8JLUUQa8JHWUAS9JHWXAS1JHGfCS1FEGvCR1lAEvSR01cMAnOSbJV5Pc0dbfkOS+JI8m+VSSV7Ty49v6gbb9tPE0XZK0krWcwb8HeLhn/XeA66pqB/B94MpWfiXw/ap6E3BdqydJOsoGCvgk24BfAn6/rQc4H7i1VbkJuLQtX9LWadsvaPUlSUfRsQPW+0/AvwJe3dZPAZ6pqufb+jywtS1vBR4HqKrnk/yg1f9e7wGT7AJ2tdXn9uy+Y/+6ejD9XsNhfe+IrvYLuts3+zVb/laSXVW1Z70HWDXgk/wycKiq9iV521Jxn6o1wLaXChYbvac9xt6qmhuoxTOmq33rar+gu32zX7MnyV5aTq7HIGfw5wH/KMnFwAnA32TxjH5TkmPbWfw2YKHVnwe2A/NJjgV+Enh6vQ2UJK3PqmPwVXVNVW2rqtOAy4DPV9U/Be4FfrVVuwK4rS3f3tZp2z9fVUecwUuSxmuYefC/CbwvyQEWx9hvbOU3Aqe08vcBVw9wrHX/F2QGdLVvXe0XdLdv9mv2DNW3eHItSd3klayS1FETD/gkFyZ5pF35OshwzlRJ8tEkh5Ls7yk7Ocnd7Srfu5Oc1MqT5MOtrw8mOWtyLV9Zku1J7k3ycJKHkrynlc9035KckOT+JF9r/fpgK+/EldldveI8yWNJvp7kgTazZOZfiwBJNiW5Nck329/aW0fZr4kGfJJjgN8DLgJOBy5Pcvok27QOHwMuPKzsauCedpXvPbz0OcRFwI522wVcf5TauB7PA++vqjcD5wJXtedm1vv2HHB+VZ0B7AQuTHIu3bkyu8tXnP98Ve3smRI5669FgP8M/FFV/TRwBovP3ej6VVUTuwFvBT7Xs34NcM0k27TOfpwG7O9ZfwTY3JY3A4+05RuAy/vVm/Ybi7Ok3tGlvgF/A/gK8BYWL5Q5tpW/+LoEPge8tS0f2+pl0m1fpj/bWiCcD9zB4jUpM9+v1sbHgNccVjbTr0UWp5x/5/B/91H2a9JDNC9e9dr0XhE7y15XVQcB2v1rW/lM9rf99/1M4D460Lc2jPEAcAi4G/gWA16ZDSxdmT2Nlq44/+u2PvAV50x3v2DxYsk/TrKvXQUPs/9afCPwXeAP2rDa7yd5FSPs16QDfqCrXjtk5vqb5ETg08B7q+qHK1XtUzaVfauqF6pqJ4tnvOcAb+5Xrd3PRL/Sc8V5b3GfqjPVrx7nVdVZLA5TXJXkH6xQd1b6dixwFnB9VZ0J/D9Wnla+5n5NOuCXrnpd0ntF7Cx7MslmgHZ/qJXPVH+THMdiuH+iqj7TijvRN4Cqegb4AoufMWxqV15D/yuzmfIrs5euOH8MuJnFYZoXrzhvdWaxXwBU1UK7PwR8lsU35ll/Lc4D81V1X1u/lcXAH1m/Jh3wXwZ2tE/6X8HilbK3T7hNo9B7Ne/hV/m+q30afi7wg6X/ik2bJGHxorWHq+pDPZtmum9JTk2yqS2/Eng7ix9szfSV2dXhK86TvCrJq5eWgV8A9jPjr8Wq+r/A40l+qhVdAHyDUfZrCj5ouBj4MxbHQf/1pNuzjvZ/EjgI/JjFd9grWRzLvAd4tN2f3OqGxVlD3wK+DsxNuv0r9Ovvs/jfvweBB9rt4lnvG/B3ga+2fu0H/k0rfyNwP3AA+B/A8a38hLZ+oG1/46T7MEAf3wbc0ZV+tT58rd0eWsqJWX8ttrbuBPa21+P/BE4aZb+8klWSOmrSQzSSpDEx4CWpowx4SeooA16SOsqAl6SOMuAlqaMMeEnqKANekjrq/wNp26/FIb8r8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ion()\n",
    "from my_util import show_graph, reset_graph\n",
    "import gym\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "env.reset()\n",
    "img = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall build the basic neural networks for the actor and critic. The structure is definitely not optimal, as I am using the same structure for both actor and critic. But since it takes a few days to train on my laptop, there is no point for me to look for the optimal structure and hyperparameter. In practice, the network structure seems definitely good enough. \n",
    "\n",
    "Since the action is between -1 and +1, we shall use `tanh` for the actor output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "n_hidden = [48, 48, 32]\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "tanh_init = tf.contrib.layers.variance_scaling_initializer(factor=16.0)\n",
    "\n",
    "def actor(X_state, name, trainable = True):\n",
    "    layer = X_state\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        for i, units in enumerate(n_hidden):\n",
    "            layer = tf.layers.dense(layer, units, activation=tf.nn.elu, name = f'hidden_{i}', kernel_initializer = he_init, trainable = trainable)\n",
    "        layer = tf.layers.dense(layer, action_size, activation=tf.nn.tanh, name = 'output', kernel_initializer = tanh_init, trainable = trainable)\n",
    "        \n",
    "    net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = scope.name)\n",
    "    vars_by_name = {var.name[len(scope.name):] : var for var in net_vars}\n",
    "    return layer, vars_by_name\n",
    "\n",
    "def critic(X_state, X_action, name, trainable = True):\n",
    "    layer = tf.concat([X_state, X_action], 1)\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        for i, units in enumerate(n_hidden):\n",
    "            layer = tf.layers.dense(layer, units, activation=tf.nn.elu, name = f'hidden_{i}', kernel_initializer = he_init, trainable = trainable)\n",
    "        layer = tf.layers.dense(layer, 1, name = 'output', trainable = trainable)\n",
    "        \n",
    "    net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = scope.name)\n",
    "    vars_by_name = {var.name[len(scope.name):] : var for var in net_vars}\n",
    "    return layer, vars_by_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create the online and copy versions of the two networks as well as the ops for copying. The critic will be partially copied, so that the Q learning is more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X_state = tf.placeholder(tf.float32, shape = [None, state_size])\n",
    "X_action = tf.placeholder(tf.float32, shape = [None, action_size])\n",
    "\n",
    "actor_online, actor_online_vars = actor(X_state, name = \"actor/online\", trainable = True)\n",
    "actor_copy, actor_copy_vars = actor(X_state, name = \"actor/copy\", trainable = False)\n",
    "\n",
    "critic_online, critic_online_vars = critic(X_state, X_action, name = \"critic/online\", trainable = True)\n",
    "critic_copy, critic_copy_vars = critic(X_state, X_action, name = \"critic/copy\", trainable = False)\n",
    "\n",
    "def copy_ops(online_vars, copy_vars, ratio = 1.0):\n",
    "    ratio = tf.constant(ratio, dtype = tf.float32)\n",
    "    return [copy_var.assign(ratio * online_vars[var_name] + (1 - ratio) * copy_vars[var_name]) for var_name, copy_var in copy_vars.items()]\n",
    "\n",
    "all_copy_ops = tf.group(*(copy_ops(actor_online_vars, actor_copy_vars, 1.0) + copy_ops(critic_online_vars, critic_copy_vars, 0.25)))\n",
    "\n",
    "actor_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = \"actor/online\")\n",
    "critic_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = \"critic/online\") \n",
    "    \n",
    "#show_graph(tf.get_default_graph())\n",
    "#tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "#tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use `deque` to store the memory buffer. For each playthrough, we choose a random `horizon`. Each entry will store a slice containing the current state, the action, the discounted total awards in the future `horizon` steps (or shorter if the game ends), and the state and action after `horizon` steps. \n",
    "\n",
    "We will not store the slices which ended by running out of time without falling, as they do not provide a final action. \n",
    "\n",
    "Since each game can take maximal 1600 steps, we might want to store only a subset of the longer games.\n",
    "\n",
    "In later games, we want to store slices of falling instances to punish it more (the goal of the task requires not falling in 100 consecutive games). In early games, we want to encourage the walker to learn maneuvers, so we punish falling less.\n",
    "\n",
    "One possibility of improvement for memorization is that, in later training, we only store slices which begin and end with nonrandom actions.(We cannot do that in early training because almost all actions are random by our design.) But that requires recording even more information for each step, and our final randomness is very low, so it seems not necessary in this setting.\n",
    "\n",
    "Another possibility of improvement is to randomly switch the role of left and right legs, since they are completely symmetric in this task, and the networks initially tend to focus completely on one side of the symmetry. But I decided not to do that, because that seems to be taking advantage of too specific knowledge about this particular task and not generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "memory_size = 200000\n",
    "memory = deque([], maxlen = memory_size)\n",
    "discount_rate = 0.99\n",
    "init_min_horizon = 50\n",
    "fin_min_horizon = 1\n",
    "horizon_length = 50\n",
    "discount_array = [discount_rate ** i for i in range(init_min_horizon + horizon_length)]\n",
    "max_store_per_game = 200\n",
    "late = False\n",
    "threshold = 200\n",
    "\n",
    "def fill_memory(mem, play, min_horizon, horizon_length):\n",
    "    horizon = random.randrange(round(min_horizon), round(min_horizon + horizon_length))\n",
    "    max_discount = discount_array[horizon]\n",
    "    tail = len(play) - 1\n",
    "    head = len(play) - 1\n",
    "    accu_reward = 0    \n",
    "    next_action = env.action_space.sample()\n",
    "    if not late and (tail >= 0):\n",
    "        if play[tail][4] and (play[tail][2] < -10): \n",
    "            play[tail][2] = -50\n",
    "    while head >= 0:\n",
    "        if tail - head == horizon:\n",
    "            accu_reward -= max_discount * play[tail][2]\n",
    "        accu_reward = accu_reward * discount_rate + play[head][2]\n",
    "\n",
    "        if tail < len(play) - 1:\n",
    "            next_action = play[tail + 1][1]\n",
    "\n",
    "        store = True\n",
    "#If the game is too long, we store a random subset     \n",
    "        if len(play) > max_store_per_game:\n",
    "            if np.random.rand() > max_store_per_game / len(play):\n",
    "                store = False\n",
    "                \n",
    "#In late training, we enforce the memorization of falling to solve the problem(100 consecutive 300+).               \n",
    "        if late:\n",
    "            if play[tail][4] and (play[tail][2] < -10): \n",
    "                store = True\n",
    "                \n",
    "#If the slice ends with a game termination, we do not store it if the game ends with timeout, since there is no next action. \n",
    "        if play[tail][4] and (play[tail][2] > -10): \n",
    "                store = False\n",
    "\n",
    "# We store the current state, action, accumulated rewards in the future horizon, future state after horizon steps, next action at future state after horizon steps, flag whether game ended, the horizon length.      \n",
    "        if store: mem.append((play[head][0], play[head][1], accu_reward, play[tail][3], next_action, play[tail][4], horizon))\n",
    "        head -= 1\n",
    "        if (tail - head > horizon):\n",
    "            tail -= 1\n",
    "    return          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the memory with completely random explorations, and save it to a pickle to speed up later initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(memory) < memory_size:\n",
    "    state = env.reset()\n",
    "    playthrough = []\n",
    "    done = False\n",
    "    while True:\n",
    "        if done:\n",
    "            fill_memory(memory, playthrough, init_min_horizon, horizon_length)\n",
    "            break\n",
    "        action = env.action_space.sample()\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        playthrough.append([state, action, reward, ob, done])\n",
    "        state = ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({'memory' : memory}, open(\"init.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "init_dict = pickle.load(open(\"init.p\", \"rb\" ))\n",
    "memory = init_dict['memory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we at least know what the training data look like. Let us write out explicitly the training ops. Here we need to pass the gradients from critic to actor (reversing the sign for gradient ascent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_iter = tf.Variable(0, trainable = False, name = 'global_iter')\n",
    "\n",
    "y_critic = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "critic_loss = tf.losses.huber_loss(y_critic, critic_online)\n",
    "\n",
    "critic_optimizer = tf.train.AdamOptimizer()\n",
    "critic_training_op = critic_optimizer.minimize(critic_loss, var_list = critic_trainable_vars, global_step = global_iter)\n",
    "critic_action_grad = tf.gradients(critic_online, X_action)[0]\n",
    "\n",
    "#proxy_grad will be used to pass on critic_action_grad, which is the gradient of critic output with respect to its action input, to the actor. The actor then computes the total gradient with respect to its weight in order to improve its rating\n",
    "proxy_grad = tf.placeholder(tf.float32, shape = [None, action_size])\n",
    "actor_optimizer = tf.train.AdamOptimizer()\n",
    "critic_actor_grad = tf.gradients(actor_online, actor_trainable_vars, - proxy_grad)\n",
    "ascend_grads = zip(critic_actor_grad, actor_trainable_vars)\n",
    "actor_training_op = actor_optimizer.apply_gradients(ascend_grads)\n",
    "\n",
    "with tf.name_scope(\"util\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    save_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    saver = tf.train.Saver(save_var_list, max_to_keep = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add randomness to the explorer's strategy. Initially it almost moves randomly, since the explorer's strategy initially is no better than the random one. The chance to explore wildly gradually drops to `min_rand`. Then the chance to choose a normal distribution around the preferred action gradually drops to `min_normal_rand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iters = 500000  #total training iterations\n",
    "min_rand = 0.05\n",
    "min_normal_rand = 0.3\n",
    "total_decay_iters = total_iters / 2\n",
    "dev = [0.03] * action_size\n",
    "\n",
    "def explore(state, iters):\n",
    "    eps = max(min_rand, 1.0 - (1.0 - min_rand) * iters / total_decay_iters)\n",
    "    if np.random.rand() < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        action = actor_copy.eval(feed_dict = {X_state: [state]})[0]\n",
    "        if np.random.rand() < max(min_normal_rand, 1.0 - (1.0 - min_normal_rand) * iters / total_decay_iters):\n",
    "            action = np.random.normal(action, dev)\n",
    "        return np.clip(action, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a utility for sampling memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memory(batch_size):\n",
    "    indices = np.random.permutation(len(memory))[:batch_size]\n",
    "    batch_state = []\n",
    "    batch_action = []\n",
    "    batch_q = []\n",
    "    for i in indices:\n",
    "        sample = memory[i]\n",
    "        batch_state.append(sample[0])\n",
    "        batch_action.append(sample[1])\n",
    "        target_q = sample[2]\n",
    "#If the slice did not end with falling, we add the discounted Q value to the accumulated rewards.        \n",
    "        if not sample[5]:\n",
    "            target_q += discount_array[sample[6]] * critic_copy.eval(feed_dict = {X_state : [sample[3]], X_action : [sample[4]]})[0][0]\n",
    "        batch_q.append([target_q])\n",
    "    return batch_state, batch_action, batch_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a utility for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make('BipedalWalker-v2')\n",
    "\n",
    "def test(round = 5):\n",
    "    score = []\n",
    "    global late\n",
    "    for i in range(round):\n",
    "        state = env2.reset()\n",
    "        total_reward = 0\n",
    "        total_steps = 0\n",
    "        while True:\n",
    "            total_steps += 1\n",
    "            action = actor_copy.eval(feed_dict = {X_state: [state]})[0]\n",
    "            ob, reward, done, _ = env2.step(action)\n",
    "            total_reward += reward\n",
    "            state = ob \n",
    "            if done:\n",
    "                score.append(total_reward)\n",
    "                if total_reward > threshold:\n",
    "                    late = True\n",
    "                print(f'steps: {total_steps}, total reward: {total_reward}')\n",
    "                break\n",
    "    return score       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be ready to train. First we set up some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_per_game = 10\n",
    "save_iters = 1000\n",
    "test_iters = 1000\n",
    "copy_iters = 200\n",
    "capture_game = 1000\n",
    "batch_size = 50\n",
    "done = True\n",
    "state = env.reset()\n",
    "playthrough = []\n",
    "episode = 0\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save regularly to observe the evoluion later. One thing to notice is that TensorFlow by default only save up to 5 most recent checkpoints, but we overwrote that in the `Saver` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    all_copy_ops.run()\n",
    "    \n",
    "    while True:\n",
    "        iters = global_iter.eval()\n",
    "        if iters >= total_iters:\n",
    "            break\n",
    "        if done:\n",
    "            if episode % capture_game == 0:\n",
    "                saver.save(sess, f\"BipedalWalker/walker_evo{episode // capture_game}/walker.ckpt\")\n",
    "            episode += 1\n",
    "            fill_memory(memory, playthrough, max(fin_min_horizon, init_min_horizon * (1 - iters / total_decay_iters) + fin_min_horizon * (iters / total_decay_iters)), horizon_length)\n",
    "            playthrough = []\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            \n",
    "            for train_i in range(train_per_game):\n",
    "                batch_state, batch_action, batch_q = sample_memory(batch_size)\n",
    "                critic_training_op.run(feed_dict = {y_critic: batch_q, X_state: batch_state, X_action: batch_action})\n",
    "                online_action = actor_online.eval(feed_dict = {X_state: batch_state})\n",
    "                actor_training_op.run(feed_dict = {X_state: batch_state, proxy_grad: critic_action_grad.eval(feed_dict = {X_state: batch_state, X_action: online_action})})\n",
    "                \n",
    "                iters = global_iter.eval()\n",
    "                \n",
    "                if iters % copy_iters == 0:\n",
    "                    all_copy_ops.run()            \n",
    "            \n",
    "                if iters % save_iters == 0:\n",
    "                    saver.save(sess, \"walker.ckpt\")\n",
    "                    print(f'{iters}/{total_iters}')\n",
    "            \n",
    "                if iters % test_iters == 0:\n",
    "                    test(1)\n",
    "            \n",
    "        #explorer explores\n",
    "        exp_action = explore(state, iters)\n",
    "        ob, reward, done, _ = env.step(exp_action)\n",
    "        playthrough.append([state, exp_action, reward, ob, done])\n",
    "        state = ob  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training above should be sufficient to achieve 300+ most of the time. Let us write a code for continuing training on a checkpoint(in case the training was interrupted or not enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque([], maxlen = memory_size)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, \"walker.ckpt\")\n",
    "    while True:\n",
    "        iters = global_iter.eval()\n",
    "#        if iters >= total_iters:\n",
    "#            break\n",
    "        if done:\n",
    "            if episode % capture_game == 0:\n",
    "                saver.save(sess, f\"BipedalWalker/walker_evo{episode // capture_game}/walker.ckpt\")\n",
    "            episode += 1\n",
    "            fill_memory(memory, playthrough, fin_min_horizon, horizon_length)\n",
    "            playthrough = []\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            \n",
    "            if len(memory) == memory_size:  #fill up memory with the current explorer\n",
    "                for train_i in range(train_per_game):\n",
    "                    batch_state, batch_action, batch_q = sample_memory(batch_size)\n",
    "                    critic_training_op.run(feed_dict = {y_critic: batch_q, X_state: batch_state, X_action: batch_action})\n",
    "                    online_action = actor_online.eval(feed_dict = {X_state: batch_state})\n",
    "                    actor_training_op.run(feed_dict = {X_state: batch_state, proxy_grad: critic_action_grad.eval(feed_dict = {X_state: batch_state, X_action: online_action})})\n",
    "                \n",
    "                    iters = global_iter.eval()\n",
    "                \n",
    "                    if iters % copy_iters == 0:\n",
    "                        all_copy_ops.run()            \n",
    "            \n",
    "                    if iters % save_iters == 0:\n",
    "                        saver.save(sess, \"walker2.ckpt\")\n",
    "                        print(f'{iters}/{total_iters}')\n",
    "            \n",
    "                    if iters % test_iters == 0:\n",
    "                        test(1)\n",
    "            \n",
    "            #explorer explores\n",
    "            exp_action = explore(state, iters)\n",
    "            ob, reward, done, _ = env.step(exp_action)\n",
    "            playthrough.append([state, exp_action, reward, ob, done])\n",
    "            state = ob  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that we get 100 consecutive 300+.(This required more training) The solution checkpoint file is in the repository. Simply run the graph constructions above before restoring the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from RL/BipedalWalker/solved.ckpt\n",
      "Episode 1 finished after 754 timesteps with total reward 308.42167874128336\n",
      "Episode 2 finished after 757 timesteps with total reward 308.3758768055089\n",
      "Episode 3 finished after 752 timesteps with total reward 308.1323551180815\n",
      "Episode 4 finished after 752 timesteps with total reward 308.74194954273815\n",
      "Episode 5 finished after 757 timesteps with total reward 308.5206721331454\n",
      "Episode 6 finished after 754 timesteps with total reward 308.8310015623297\n",
      "Episode 7 finished after 749 timesteps with total reward 309.35850266936376\n",
      "Episode 8 finished after 758 timesteps with total reward 307.9368042670929\n",
      "Episode 9 finished after 749 timesteps with total reward 309.12726146758587\n",
      "Episode 10 finished after 750 timesteps with total reward 308.7372660422396\n",
      "Episode 11 finished after 754 timesteps with total reward 309.3206089217363\n",
      "Episode 12 finished after 761 timesteps with total reward 307.4690309108119\n",
      "Episode 13 finished after 753 timesteps with total reward 308.0690988743178\n",
      "Episode 14 finished after 759 timesteps with total reward 308.54537599597467\n",
      "Episode 15 finished after 755 timesteps with total reward 307.86210571248773\n",
      "Episode 16 finished after 758 timesteps with total reward 308.9625024284218\n",
      "Episode 17 finished after 754 timesteps with total reward 308.98501281363787\n",
      "Episode 18 finished after 756 timesteps with total reward 307.7589920439028\n",
      "Episode 19 finished after 753 timesteps with total reward 308.4358441377274\n",
      "Episode 20 finished after 747 timesteps with total reward 308.7442756925925\n",
      "Episode 21 finished after 751 timesteps with total reward 308.9804326757726\n",
      "Episode 22 finished after 760 timesteps with total reward 307.47581351335884\n",
      "Episode 23 finished after 762 timesteps with total reward 307.81238202776126\n",
      "Episode 24 finished after 755 timesteps with total reward 307.8555264198897\n",
      "Episode 25 finished after 757 timesteps with total reward 308.31859502218765\n",
      "Episode 26 finished after 749 timesteps with total reward 307.8728174846714\n",
      "Episode 27 finished after 755 timesteps with total reward 307.2972488017263\n",
      "Episode 28 finished after 751 timesteps with total reward 309.3707964622252\n",
      "Episode 29 finished after 753 timesteps with total reward 308.57314306626625\n",
      "Episode 30 finished after 756 timesteps with total reward 307.9414607454687\n",
      "Episode 31 finished after 761 timesteps with total reward 308.35169268442434\n",
      "Episode 32 finished after 758 timesteps with total reward 307.8756733159917\n",
      "Episode 33 finished after 754 timesteps with total reward 309.0413095883055\n",
      "Episode 34 finished after 761 timesteps with total reward 309.31846560464527\n",
      "Episode 35 finished after 754 timesteps with total reward 307.5588464922208\n",
      "Episode 36 finished after 753 timesteps with total reward 308.4216147004565\n",
      "Episode 37 finished after 761 timesteps with total reward 308.71324397418323\n",
      "Episode 38 finished after 751 timesteps with total reward 308.6665278194519\n",
      "Episode 39 finished after 752 timesteps with total reward 308.77653295785154\n",
      "Episode 40 finished after 755 timesteps with total reward 307.91231143696274\n",
      "Episode 41 finished after 757 timesteps with total reward 307.9341826501109\n",
      "Episode 42 finished after 756 timesteps with total reward 308.77885320977293\n",
      "Episode 43 finished after 755 timesteps with total reward 307.7816715771535\n",
      "Episode 44 finished after 758 timesteps with total reward 309.111092113982\n",
      "Episode 45 finished after 759 timesteps with total reward 308.2672552302986\n",
      "Episode 46 finished after 756 timesteps with total reward 307.9509408586695\n",
      "Episode 47 finished after 754 timesteps with total reward 308.35413101347376\n",
      "Episode 48 finished after 758 timesteps with total reward 308.1616460884765\n",
      "Episode 49 finished after 762 timesteps with total reward 308.2919301132932\n",
      "Episode 50 finished after 755 timesteps with total reward 308.64394989135565\n",
      "Episode 51 finished after 754 timesteps with total reward 309.1466307717986\n",
      "Episode 52 finished after 746 timesteps with total reward 309.62135404372344\n",
      "Episode 53 finished after 758 timesteps with total reward 308.91180128684755\n",
      "Episode 54 finished after 756 timesteps with total reward 307.5558639799553\n",
      "Episode 55 finished after 757 timesteps with total reward 308.6807590682029\n",
      "Episode 56 finished after 753 timesteps with total reward 307.75825591621503\n",
      "Episode 57 finished after 759 timesteps with total reward 309.2683767346838\n",
      "Episode 58 finished after 753 timesteps with total reward 308.9343907734614\n",
      "Episode 59 finished after 754 timesteps with total reward 308.4449625171752\n",
      "Episode 60 finished after 754 timesteps with total reward 308.8912626289757\n",
      "Episode 61 finished after 751 timesteps with total reward 309.314185535244\n",
      "Episode 62 finished after 755 timesteps with total reward 308.91903936621225\n",
      "Episode 63 finished after 756 timesteps with total reward 308.2430230698569\n",
      "Episode 64 finished after 756 timesteps with total reward 309.08631972502417\n",
      "Episode 65 finished after 757 timesteps with total reward 309.6275731761682\n",
      "Episode 66 finished after 753 timesteps with total reward 308.09884374855835\n",
      "Episode 67 finished after 755 timesteps with total reward 309.1037153244655\n",
      "Episode 68 finished after 760 timesteps with total reward 308.8912521539014\n",
      "Episode 69 finished after 754 timesteps with total reward 308.44364329101114\n",
      "Episode 70 finished after 752 timesteps with total reward 309.36718805742674\n",
      "Episode 71 finished after 763 timesteps with total reward 307.87197786153075\n",
      "Episode 72 finished after 762 timesteps with total reward 308.1942639094734\n",
      "Episode 73 finished after 756 timesteps with total reward 308.53136145458495\n",
      "Episode 74 finished after 756 timesteps with total reward 308.2138783670385\n",
      "Episode 75 finished after 755 timesteps with total reward 309.2641102116001\n",
      "Episode 76 finished after 753 timesteps with total reward 308.4664518073919\n",
      "Episode 77 finished after 756 timesteps with total reward 309.663845829015\n",
      "Episode 78 finished after 754 timesteps with total reward 308.1527654245594\n",
      "Episode 79 finished after 759 timesteps with total reward 308.4296783026691\n",
      "Episode 80 finished after 751 timesteps with total reward 307.35518831455516\n",
      "Episode 81 finished after 751 timesteps with total reward 308.682009556095\n",
      "Episode 82 finished after 757 timesteps with total reward 308.982398001763\n",
      "Episode 83 finished after 755 timesteps with total reward 308.81451124164437\n",
      "Episode 84 finished after 756 timesteps with total reward 308.5158695051713\n",
      "Episode 85 finished after 751 timesteps with total reward 309.45301218176087\n",
      "Episode 86 finished after 755 timesteps with total reward 309.09683184572606\n",
      "Episode 87 finished after 752 timesteps with total reward 309.13111129342025\n",
      "Episode 88 finished after 754 timesteps with total reward 309.0414776060807\n",
      "Episode 89 finished after 754 timesteps with total reward 308.19851558607417\n",
      "Episode 90 finished after 758 timesteps with total reward 308.1336692304591\n",
      "Episode 91 finished after 760 timesteps with total reward 308.2134689682377\n",
      "Episode 92 finished after 753 timesteps with total reward 308.7759529931491\n",
      "Episode 93 finished after 751 timesteps with total reward 307.8593264318804\n",
      "Episode 94 finished after 757 timesteps with total reward 309.2371107218113\n",
      "Episode 95 finished after 749 timesteps with total reward 308.87420639267725\n",
      "Episode 96 finished after 753 timesteps with total reward 308.9936277960906\n",
      "Episode 97 finished after 761 timesteps with total reward 307.9182115799237\n",
      "Episode 98 finished after 751 timesteps with total reward 309.09999042011196\n",
      "Episode 99 finished after 753 timesteps with total reward 307.72554508062694\n",
      "Episode 100 finished after 762 timesteps with total reward 307.6299898012961\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow as tf\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5  \n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, \"solved.ckpt\")\n",
    "    for i_episode in range(100):\n",
    "        observation = env.reset()\n",
    "        total_reward = 0\n",
    "        for t in range(2000):\n",
    "            #env.render()\n",
    "            action = actor_copy.eval(feed_dict = {X_state: [observation]})[0]\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print(f\"Episode {i_episode + 1} finished after {t+1} timesteps with total reward {total_reward}\")\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We record the evolution of the walker into videos. To annotate the episode number, we modify the `video_recorder.py` file in the `gym` package. The replacement file can be found in my GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "env = wrappers.Monitor(env, \"BipedalWalker/evolution\", force = True, video_callable = lambda x: True)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            saver.restore(sess, f\"BipedalWalker/walker_evol{i}/walker.ckpt\")\n",
    "            for i_episode in range(1):\n",
    "                observation = env.reset()\n",
    "                total_reward = 0\n",
    "                for t in range(2000):\n",
    "                    env.render()\n",
    "                    action = actor_copy.eval(feed_dict = {X_state: [observation]})[0]\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "            i += 1\n",
    "        except:\n",
    "            break\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.85,
   "position": {
    "height": "144.45px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
