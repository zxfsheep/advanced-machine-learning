{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Learning from Simulated and Unsupervised Images through Adversarial Training**](https://arxiv.org/pdf/1612.07828.pdf): The Facebook paper on training gaze images. An external simulator generates synthetic gaze images, and two neural networks form a [**GAN**](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) to transform them to realistic images using real images, while preserving labels. \n",
    "\n",
    "  The first network, the Refiner, turns a synthetic image to a realistic image, trained by a 2-component loss function. One term corresponds to how well the other network, the Discriminator, can tell if the output is real or not. The other term is a grid-based l1-norm difference between the input and output, in order to preserve the label.\n",
    "\n",
    "  The second network, the Discriminator, is trained using batches of mixtures of real images and memories of refined images, by the logistic loss of how well it can tell if an image is real or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**R-CNN**](https://arxiv.org/pdf/1311.2524.pdf): Using prior RoI (region of interest) proposer, run CNN on each proposed region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Fast R-CNN**](https://arxiv.org/abs/1504.08083): First run a CNN on the entire picture, and then use prior RoI (region of interest) proposer on the resulted feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Faster R-CNN**](https://arxiv.org/abs/1506.01497): Instead of using an external RoI proposer, run a parallel branch network on top of shared CNN. It slides a fixed size window, and propose region based on a fixed set of anchors (rectangles of different scales and ratios).\n",
    "\n",
    "  On top of these models, there are heads doing object classification and also bounding box regression, so the cost function is multi-part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Fully Convolutional Networks**](https://arxiv.org/pdf/1411.4038.pdf): For semantic segmentation with dense output. This NN has only convolutional layers, which instead of outputing a single prediction, outputs a pixelwise prediction. The output can be very coarse, which can be then upsampled by transposed convolution (deconvolution), and learned as well. The NN can reuse the convolutional part of classification NN such as VGGnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Mask R-CNN**](https://arxiv.org/abs/1703.06870): Compared to Faster R-CNN, also put a FCN along side the previous heads, which is pixel to pixel, and generates a binary mask for instance segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**YOLO**](https://pjreddie.com/media/files/papers/YOLOv3.pdf): You only look once, which divides picture into a grid, and for each cell implements bounding box regression, objection detection and objection classification, and combine these together. The drawback is difficulty to capture a group of small objects. The improved version borrows techniques from models above I believe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Focal Loss**](https://arxiv.org/abs/1708.02002): This is a simple one-step approach at object recognition that performs as well as two-step approaches but with much better efficiency. It gets rid of the region proposal component, and instead work on a large number of raw candidates. The main innovation is the focal loss, which compared to usual cross-entropy loss, tunes down the weights of well-classified samples. The exact formula to use is not essential in their experiment. The benefit is that the model will automatically focus on hard examples, in foreground or background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
