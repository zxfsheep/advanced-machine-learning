* [**Word2Vec**](https://arxiv.org/pdf/1301.3781.pdf): The goal is to learn word embedding as vectors. There are two types of auxiliary training tasks: the **Continuous Bag-of-Words model (CBOW)** and the **Skip-Gram** model. CBOW predicts target from context, and Skip-Gram vice versa. For larger dataset, Skip-Gram performs better. The naive approach is to let the NN output a probability vector for all words, and minimize the logloss. However in practice, we use Monte Carlo to randomly sample wrong words to do a binary classification with the correct word. This is called **Noise-Contrastive Training**.

  The output of NN is the embedding. In order to [make prediction](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) of a context word (in Skip-Gram model), the model also trains a weight and bias for the entire vocabulary. Suppose the embedding vector has dimension $n$, then the weight of each word also has dimension $n$, and to make prediction, the context word embedding is multiplied by the weight matrix, then adding bias term. These are passed to a sigmoid function to output probability. Therefore, the weight and bias can be thought of as describing how each context word **interacts** with all other words, while the embedding behaves like an ID of each word.

* [**GloVe: Global Vectors for Word Representation**](http://www.aclweb.org/anthology/D14-1162): This is a word-to-vector model that combines the global count vector technique (e.g. **latent semantic analysis (LSA)**) and the local context window method (as mentioned above in **Word2Vec**). It is still trying to learn a linear embedding of the words, using global counts of context window occurrence. The main difference from the **Hyperspace Analogue to Language (HAL)** is that it is deduced from the ratios of conditional probabilities, so that the frequent words do not distort the resulted embedding. The cost function is the following:

  $$J = \sum_{i,j = 1}^V f(X_{ij})(w_i^T \tilde w_j+b_i+\tilde b_j - \log X_{ij})^2$$

  where $V$ is the size of the vocabulary, $X_{ij}$ is the number of times word $j$ appears in the context windows of word $i$, $w_i$ is the word vector of word $i$, $\tilde w_j$ is the context word vector of word $j$, $b$'s are bias terms, and $f$ is a weight function for co-occurrences. A heuristic analysis shows that the time complexity of GloVe is $O(|C|^{0.8})$ where $C$ is the corpus.

* **Distant supervision for relation extraction**ï¼šThis is a weak supervision technique developed by [Mintz et al.](https://web.stanford.edu/~jurafsky/mintz.pdf) that utilizes an established relation instance database.  Given a text corpus, if an entity pair (for example, "Michael Jackson" and "Gary")  existent in the database appear in the same sentence, the DS assumption is that the sentence is expressing the "place of birth" relation. The lexical and syntactic features are extracted from the sentence for this entity pair. All such extracted features are then fed to a classifier, which will be used to predict new relation instances.

  One issue with this basic version is that the extracted features are rather noisy. [Takamatsu et al.](https://dl.acm.org/citation.cfm?id=2390626) proposed a technique based on generative models to reduce the wrong relation labels in the data before feeding to the classifier, and thus reducing the final extraction bias. Their motivation is that, some patterns such as "moved from" often appear for an entity pair if "born in" also appears for it. Even though "moved from" does not express a "place of birth" relation, a basic distant supervision model will tend to believe that it does. The generative model takes into account of the overlapping information between patterns in the unlabeled data and attempts to model the labeling result of distant supervision before training. The fitted hidden parameters of the generative model then can be treated as confidence score of each pattern for whether it actually expresses certain relation.

* [**Seq2Seq**](https://arxiv.org/abs/1703.03906): This is an encoder-decoder framework that not only works for machine translation, but also for other similar tasks that involve semantic transformation/summarization. The basic idea is to use RNN to first encode the original information, and then use another RNN to output(decode) the desired target information. The RNN network uses long-term memory cells such as **LSTM cell** or **GRU cell**. 

  For translation, the words are first embedded as vectors, using techniques mention above, and fed to the encoder in reverse order, since the beginning of the sentence is the usually the first to translate. During training, the desired output is fed to the decoder lagging one step. In prediction, the output in the previous step is fed back to the decoder. To deal with varied length, an End-of-Sentence token is added to each sentence. A **Bi-directional RNN** is also used to make information flow not just in one direction. Each output is also a vector and we compute its dot product with the target word vectors in the vocabulary. This is transformed into a probability distribution via softmax function. A [**sampled softmax technique**](https://arxiv.org/pdf/1412.2007.pdf) can be used to efficiently train it by minimizing a sampled softmax loss.

  Another improvement to improve accuracy is the **attention mechanism**. An attention network, which takes the context and hidden states as input, guides the decoder to peek into the input sentence and focus on relevant pieces.

* [**Semi-Supervised Text Classification using EM(Expectation-Maximization)**](https://www.cs.cmu.edu/~tom/pubs/NigamEtAl-bookChapter.pdf):  The base model is the **mixture of multinomials** generative model, which models a bag of words ignoring the ordering. Given a set of parameters $\theta$, the model first selects a class label $c_j$ with probability measure $P(c_j|\theta)$, and then generates the document $x_i$ (represented by its word count vector) according to the naive Bayes assumption:

  $$P(x_i|c_j, \theta)\propto P(|x_i|)\prod_{w_t\in \chi}P(w_t|c_j, \theta)^{x_{i,t}}$$

  where $\chi$ is the set of vocabulary and $x_{i,t}$ are the word count vector components. 

  The prior distribution is assumed to be product of Dirichlet distribution. In the supervised case, naive Bayes uses maximum a posteriori (MAP) estimate to obtain $\theta$ based on the prior and training data. Then we can use $\theta$ for classification.

  In the semi-supervised setting, ie. with a mixture of labeled and unlabeled data, we first use the labeled data to initialize a choice of parameter using naive Bayes. Then we iterate Expectation and Maximization steps, where the Expectation step uses the current parameter to produce class probabilities for unlabeled data, and the Maximization step use all labeled and unlabeled data to obtain a new $\theta$. More specifically, each unlabeled sample is treated as a mixture of samples with different classes weighted by class probabilities. EM procedure is iterated until convergence. This only converges to a local maximum. 

  **Warning:** Using unlabeled data could degrade the learning performance. This happens especially when the assumed model is incorrect. However, when there are too few labeled data, unlabeled data can help reduce variance, and thus improve performance even if it has a larger bias.

  So far classes and mixtures have 1-to-1 correspondence. This is not always sufficiently expressive, for example when a *negative* class contains a wide range of different subtopics. Therefore the model is strengthened to model many-to-1 correspondence from mixture to class, where each class also has a probability measure on different subtopics. Now even for labeled data, an EM iteration is needed because subtopics are unknown. 

  Furthermore, in order to avoid local maxima, **Deterministic Annealing** can be used. This technique starts with a target function almost convex, in which case the global maximum is easy to find, and gradually transition to the likelihood function that we actually want to maximize, where each EM procedure is initialized with the parameters found in the previous step. This hopefully will lead to a final result close to a global maximum. However this is not always the case, as the iterations might not be able to correct the wrong class-subtopic correspondences decided earlier. One can try to correct the class-subtopic correspondence by human inspection or by the basic EM algorithms described above.

---



## Basic concepts:

* **tf-idf**: The product of two statistics, term frequency and inverse document frequency, is a importance weight given to a word in a set of document. They can be defined in various ways, but basically term frequency describes how often it appears, and inverse document frequency describes how many documents contain the word. The intuition is that if every document contains the word, it might be so common that it does not provide much specific information.
