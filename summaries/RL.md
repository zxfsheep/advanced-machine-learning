* **Policy gradients**: on-policy model that uses a NN to output probabilities for possible actions. An action is chosen at random. We will treat this chosen action as the "target" action and compute the gradient that moves towards this action. The score of the action is the discounted sum of credits for the entire future. We run a batch of episodes, and find the average normalized score of each action. Then we multiply the gradient associated to each action by that score, and compute the average gradient to update the NN. In more sophisticated model, this normalized score is replaced by **advantage** function, i.e. the excess of rewards choosing some action versus average over all actions.

* **Deep Q-learning**: $Q(s,a)$ denotes the score of choosing action $a$ at state $s$. This is an off-policy model, where we keep a $Q^*(s,a)$ on the side as target and update once in a while. We keep a large pool of replay memories, and take a subset of snapshots. An explorer keeps playing game using $Q(s,a)$, thus generating memories, but still has a small probability $\varepsilon$ to choose suboptimal action. Every once in a while, we take a batch of snapshots in the memory (not necessarily generated by the current $Q(s,a)$), and find the difference between $Q(s,a)$'s and the freshly computed target $Q$ based on the next state $s'$ and $Q^*(s',a')$ using the recursive formula, and minimize the Huber loss. 

* **Double DQN**: One way to improve DQN above is to let the current $Q$ choose $a'$ for target $Q$. This decouples the valuation and action choice.

* [**Dueling DQN**](https://arxiv.org/pdf/1511.06581.pdf): Use two networks to compute state value $V$ and advantage function $A$ separately, and add them to get $Q$. This decouples the inherent value of a state from the actions.

* **Deep Deterministic Policy Gradient (DDPG)**: To generalize to continuous action space, this uses the **actor-critic** model. The actor NN generates the action, and the critic evaluates the state-action value, say $Q$ value (I have also seen versions using $V$ value, ie. state value). To encourage exploration, noise is added to action. We also keep a replay memory pool. As in DQN, keep an actor-critic pair on the side as "target", which get updated once in a while (partially). Sampling a batch of memories, the critic is trained by minimizing its difference from the recursively computed $Q$ value using the target actor-critic. The actor's policy is updated as in policy gradient to maximize the current $Q$ value. These are trained together to achieve better actions as well as more accurate valuations.

* [**A3C**](https://arxiv.org/pdf/1602.01783.pdf): The first A stands for Asynchronous. It is basically actor-critic where multiple actors run simultaneously. This follows from the observation that, in the DDPG, since we keep the target actor-critic frozen for a period of time, there is no reason to have only one actor. More actors running parallel not only improves performance, but also creates diversity. They update the target actor-critic separately. An entropy term can be added to the loss to encourage exploration.

  An actor does not have to run to the end. It can run a small number of steps, and calculate the advantage using the critic. The advantage estimator is to subtract the current value from discounted rewards plus discounted final value. See [this](https://arxiv.org/pdf/1506.02438.pdf) for **generalized advantage estimator**. This combines actual rewards and function value to **trade off bias and variance**. It is always a big challenge to trade off these two. Usually actual rewards gives lower bias, and function value provides lower variance.

  Also, these actors do need even need to be trained, since they are short-lived. The gradients are directly applied to the global actor-critic in real time. So this is basically an **on-policy** scheme.

  *On the use of LSTM*: these can be used to deal with a history of data. The longer history provided, the more accurate the result should be. In generating memories, a single time step is fine. For training, a longer time series can be plug in to generate the actions/values for all of them, i.e. from the outputs of the `dynamic_rnn` along the time dimension.

* [**Trust Region Policy Optimization (TRPO)**](https://arxiv.org/pdf/1502.05477.pdf): This is an improvement of policy gradient based on by a formula of Kakade & Langford: $\eta(\pi) = \eta_0(\pi) + \sum_s\rho_\pi(s)\sum_a\pi(a|s) A_{\pi_0}(s,a)$
  where $\eta(\pi)$ is the expected discounted total rewards, $\pi$ is a new policy, $\pi_0$ is the old policy, $\rho_\pi(s)$ is the discounted total visit times, $A_{\pi_0}$ is the advantage function (taking action $a$ over taking all actions with policy $\pi_0$). If we approximate $\rho_\pi$ by $\rho_{\pi_0}$, and try to maximize $\eta(\pi)$, there is some "trusted region" within which the true $\eta(\pi)$ has a lower bound close to the maximized value. The difference can be described  by the KL-divergence between old and new policy. 

  The actual algorithm, instead of solving an unconstrained maximization to find a lower bound, uses a constraint on KL convergence. I.e. we maximize without KL-divergence error, and add a constraint upper bound of KL-divergence error.

  Treating $ \sum_s\rho_\pi(s)$ as another expectation, and $\sum_a\pi(a|s) = E_{a\sim q}\frac{\pi(a|s)}{q(a|s)}$, where $q$ can be taken as $\pi_0$, the goal becomes maximizing $E_{\cdots} \frac{\pi(a|s)}{\pi_0(a|s)}Q_{\pi_0}(s,a)$ where $Q$ is the action-state value part. Finally Monte Carlo estimates expectation and $Q$ is also estimated either on a single path or multiple rollouts. Finally use conjugate gradient to find the solution.

* [**ACER**](https://arxiv.org/pdf/1611.01224.pdf): This can be thought of as an **off-policy** version of A3C, although the actual algorithm alternates between on-policy and several off-policy runs. So it uses memory replay. There are several techniques involved:

  In order to weight the memory according to how close they are to the current policy, instead of treating them all equally. ACER separates the weighted gradient into a truncated term and a correction term. The correction term uses the current NN $Q$ value, called **truncation with bias correction**. The truncated term uses **Retrace** to estimate current $Q$, which reduces bias in policy gradient, and also trains current NN $Q$.

  Then ACER uses a version of TRPO, where a running average policy network is maintained, and the update should not deviate from it too much. Another trick is to perform TRPO at the level of output probabilities, and backpropagates to the actual NN parameters. 

  In the discrete action case, NN outputs $Q$ and integrated over actions to obtain $V$. In the continuous action case, ACER uses a Stochastic Dueling Network similar to Dueling DQN above, where one network estimates $V$ value, and another network estimates $\tilde{Q}(s,a)$ value which models $V(s)+A(s,a)$ minus the average of a group of $A$'s sampled with distribution according to the policy NN. This $\tilde{Q}$ provides a training target for $V$. For trust region, the output probabilities are modeled by Gaussian distribution instead of discrete probability vector.

* [**Proximal Policy Optimization(PPO)**](https://arxiv.org/pdf/1707.06347.pdf): This is a variation of TRPO that is much simpler. Instead of using constraint of KL-divergence to stay in trusted region, the objective to maximize is clipped: $\frac{\pi(a|s)}{\pi_0(a|s)}$ is clipped at $[1-\varepsilon, 1+\varepsilon]$, and the lower one is kept to be pessimistic.

  The training uses the Actor-Critic style. The actor (policy) and critic (value) share NN parameters. $N$ actors rollout for $T$ steps, and critic estimate the advantage function. The entire score to maximize by SGD has three parts: the clipped objective, the negative value function error, and entropy to encourage exploration.

  Note: This is an **on-policy algorithm**, although does not have to be in practice by [this](https://github.com/openai/baselines/issues/316).

* [**Evolution**](https://arxiv.org/abs/1703.03864): This one is special because there is no backpropagation. Starting with some parameters of the NN or any action model, a group of random parameters nearby are sampled, and the entire group is reweighted according to their score and averaged. So this is like an evolution procedure. It introduces randomness in the parameter space, instead of the action space. To make samples produce more distinct outcomes, virtual batch normalization is used, meaning a fixed reference batch is set aside to normalize each sample.

