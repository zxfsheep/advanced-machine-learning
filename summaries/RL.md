* **Policy gradients**: on-policy model that uses a NN to output probabilities for possible actions. An action is chosen at random. We will treat this chosen action as the "target" action and compute the gradient that moves towards this action. The score of the action is the discounted sum of credits for the entire future. We run a batch of episodes, and find the average normalized score of each action. Then we multiply the gradient associated to each action by that score, and compute the average gradient to update the NN. In more sophisticated model, this normalized score is replaced by **advantage** function, i.e. the excess of rewards choosing some action versus average over all actions.

* **Deep Q-learning**: <img src="/summaries/tex/a18656976616796e481e7c608b8a2b40.svg?invert_in_darkmode&sanitize=true" align=middle width=49.48137479999998pt height=24.65753399999998pt/> denotes the score of choosing action <img src="/summaries/tex/44bc9d542a92714cac84e01cbbb7fd61.svg?invert_in_darkmode&sanitize=true" align=middle width=8.68915409999999pt height=14.15524440000002pt/> at state <img src="/summaries/tex/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode&sanitize=true" align=middle width=7.7054801999999905pt height=14.15524440000002pt/>. This is an off-policy model, where we keep a <img src="/summaries/tex/842b11457ed5ff9ab2dd00f530758441.svg?invert_in_darkmode&sanitize=true" align=middle width=57.03846389999999pt height=24.65753399999998pt/> on the side as target and update once in a while. We keep a large pool of replay memories, and take a subset of snapshots. An explorer keeps playing game using <img src="/summaries/tex/a18656976616796e481e7c608b8a2b40.svg?invert_in_darkmode&sanitize=true" align=middle width=49.48137479999998pt height=24.65753399999998pt/>, thus generating memories, but still has a small probability <img src="/summaries/tex/9ae7733dac2b7b4470696ed36239b676.svg?invert_in_darkmode&sanitize=true" align=middle width=7.66550399999999pt height=14.15524440000002pt/> to choose suboptimal action. Every once in a while, we take a batch of snapshots in the memory (not necessarily generated by the current <img src="/summaries/tex/a18656976616796e481e7c608b8a2b40.svg?invert_in_darkmode&sanitize=true" align=middle width=49.48137479999998pt height=24.65753399999998pt/>), and find the difference between <img src="/summaries/tex/a18656976616796e481e7c608b8a2b40.svg?invert_in_darkmode&sanitize=true" align=middle width=49.48137479999998pt height=24.65753399999998pt/>'s and the freshly computed target <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> based on the next state <img src="/summaries/tex/675c2f5707a1fa7050c12adc1872ba32.svg?invert_in_darkmode&sanitize=true" align=middle width=11.49544109999999pt height=24.7161288pt/> and <img src="/summaries/tex/66d2128ab46ccd9ec45608a85cf67f31.svg?invert_in_darkmode&sanitize=true" align=middle width=66.26221304999999pt height=24.7161288pt/> using the recursive formula, and minimize the Huber loss. 

* **Double DQN**: One way to improve DQN above is to let the current <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> choose <img src="/summaries/tex/4fc63d27626433f23e36eca761bac52b.svg?invert_in_darkmode&sanitize=true" align=middle width=12.47911664999999pt height=24.7161288pt/> for target <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/>. This decouples the valuation and action choice.

* [**Dueling DQN**](https://arxiv.org/pdf/1511.06581.pdf): Use two networks to compute state value <img src="/summaries/tex/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode&sanitize=true" align=middle width=13.242037049999992pt height=22.465723500000017pt/> and advantage function <img src="/summaries/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> separately, and add them to get <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/>. This decouples the inherent value of a state from the actions.

* **Deep Deterministic Policy Gradient (DDPG)**: To generalize to continuous action space, this uses the **actor-critic** model. The actor NN generates the action, and the critic evaluates the state-action value, say <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> value (I have also seen versions using <img src="/summaries/tex/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode&sanitize=true" align=middle width=13.242037049999992pt height=22.465723500000017pt/> value, ie. state value). To encourage exploration, noise is added to action. We also keep a replay memory pool. As in DQN, keep an actor-critic pair on the side as "target", which get updated once in a while (partially). Sampling a batch of memories, the critic is trained by minimizing its difference from the recursively computed <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> value using the target actor-critic. The actor's policy is updated as in policy gradient to maximize the current <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> value. These are trained together to achieve better actions as well as more accurate valuations.

* [**A3C**](https://arxiv.org/pdf/1602.01783.pdf): The first A stands for Asynchronous. It is basically actor-critic where multiple actors run simultaneously. This follows from the observation that, in the DDPG, since we keep the target actor-critic frozen for a period of time, there is no reason to have only one actor. More actors running parallel not only improves performance, but also creates diversity. They update the target actor-critic separately. An entropy term can be added to the loss to encourage exploration.

  An actor does not have to run to the end. It can run a small number of steps, and calculate the advantage using the critic. The advantage estimator is to subtract the current value from discounted rewards plus discounted final value. See [this](https://arxiv.org/pdf/1506.02438.pdf) for **generalized advantage estimator**. This combines actual rewards and function value to **trade off bias and variance**. It is always a big challenge to trade off these two. Usually actual rewards gives lower bias, and function value provides lower variance.

  Also, these actors do need even need to be trained, since they are short-lived. The gradients are directly applied to the global actor-critic in real time. So this is basically an **on-policy** scheme.

  *On the use of LSTM*: these can be used to deal with a history of data. The longer history provided, the more accurate the result should be. In generating memories, a single time step is fine. For training, a longer time series can be plug in to generate the actions/values for all of them, i.e. from the outputs of the `dynamic_rnn` along the time dimension.

* [**Trust Region Policy Optimization (TRPO)**](https://arxiv.org/pdf/1502.05477.pdf): This is an improvement of policy gradient based on by a formula of Kakade & Langford: <img src="/summaries/tex/9cd73f80aa22026eb53f5e739297236c.svg?invert_in_darkmode&sanitize=true" align=middle width=315.04670834999996pt height=24.657735299999988pt/>
  where <img src="/summaries/tex/0dbce69b8c8447ed21986f6cb4672cd2.svg?invert_in_darkmode&sanitize=true" align=middle width=31.49745719999999pt height=24.65753399999998pt/> is the expected discounted total rewards, <img src="/summaries/tex/f30fdded685c83b0e7b446aa9c9aa120.svg?invert_in_darkmode&sanitize=true" align=middle width=9.96010619999999pt height=14.15524440000002pt/> is a new policy, <img src="/summaries/tex/8a18488bc84c9dcd2e0c397c94d82b34.svg?invert_in_darkmode&sanitize=true" align=middle width=15.922859699999991pt height=14.15524440000002pt/> is the old policy, <img src="/summaries/tex/2bfd8d314fceae4b81918588f2ff63fc.svg?invert_in_darkmode&sanitize=true" align=middle width=37.91167379999999pt height=24.65753399999998pt/> is the discounted total visit times, <img src="/summaries/tex/951fdd3585d08ec9933555be1e5172b5.svg?invert_in_darkmode&sanitize=true" align=middle width=25.609592249999988pt height=22.465723500000017pt/> is the advantage function (taking action <img src="/summaries/tex/44bc9d542a92714cac84e01cbbb7fd61.svg?invert_in_darkmode&sanitize=true" align=middle width=8.68915409999999pt height=14.15524440000002pt/> over taking all actions with policy <img src="/summaries/tex/8a18488bc84c9dcd2e0c397c94d82b34.svg?invert_in_darkmode&sanitize=true" align=middle width=15.922859699999991pt height=14.15524440000002pt/>). If we approximate <img src="/summaries/tex/9d63e3051b5c0005c143d13d266cab14.svg?invert_in_darkmode&sanitize=true" align=middle width=16.59884489999999pt height=14.15524440000002pt/> by <img src="/summaries/tex/f40b3d271b53b7f4832017710583b1b0.svg?invert_in_darkmode&sanitize=true" align=middle width=21.77967824999999pt height=14.15524440000002pt/>, and try to maximize <img src="/summaries/tex/0dbce69b8c8447ed21986f6cb4672cd2.svg?invert_in_darkmode&sanitize=true" align=middle width=31.49745719999999pt height=24.65753399999998pt/>, there is some "trusted region" within which the true <img src="/summaries/tex/0dbce69b8c8447ed21986f6cb4672cd2.svg?invert_in_darkmode&sanitize=true" align=middle width=31.49745719999999pt height=24.65753399999998pt/> has a lower bound close to the maximized value. The difference can be described  by the KL-divergence between old and new policy. 

  The actual algorithm, instead of solving an unconstrained maximization to find a lower bound, uses a constraint on KL convergence. I.e. we maximize without KL-divergence error, and add a constraint upper bound of KL-divergence error.

  Treating <img src="/summaries/tex/85bc90e74eea9f06481722dbd3322bbf.svg?invert_in_darkmode&sanitize=true" align=middle width=65.02926704999999pt height=24.657735299999988pt/> as another expectation, and <img src="/summaries/tex/c82db5fd78de74cbc7f25d51cd9b69e9.svg?invert_in_darkmode&sanitize=true" align=middle width=168.05191589999998pt height=33.20539859999999pt/>, where <img src="/summaries/tex/d5c18a8ca1894fd3a7d25f242cbe8890.svg?invert_in_darkmode&sanitize=true" align=middle width=7.928106449999989pt height=14.15524440000002pt/> can be taken as <img src="/summaries/tex/8a18488bc84c9dcd2e0c397c94d82b34.svg?invert_in_darkmode&sanitize=true" align=middle width=15.922859699999991pt height=14.15524440000002pt/>, the goal becomes maximizing <img src="/summaries/tex/de1a209b2ccc485db96726a109f2a2a6.svg?invert_in_darkmode&sanitize=true" align=middle width=134.6357463pt height=33.20539859999999pt/> where <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> is the action-state value part. Finally Monte Carlo estimates expectation and <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> is also estimated either on a single path or multiple rollouts. Finally use conjugate gradient to find the solution.

* [**ACER**](https://arxiv.org/pdf/1611.01224.pdf): This can be thought of as an **off-policy** version of A3C, although the actual algorithm alternates between on-policy and several off-policy runs. So it uses memory replay. There are several techniques involved:

  In order to weight the memory according to how close they are to the current policy, instead of treating them all equally. ACER separates the weighted gradient into a truncated term and a correction term. The correction term uses the current NN <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> value, called **truncation with bias correction**. The truncated term uses **Retrace** to estimate current <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/>, which reduces bias in policy gradient, and also trains current NN <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/>.

  Then ACER uses a version of TRPO, where a running average policy network is maintained, and the update should not deviate from it too much. Another trick is to perform TRPO at the level of output probabilities, and backpropagates to the actual NN parameters. 

  In the discrete action case, NN outputs <img src="/summaries/tex/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=22.465723500000017pt/> and integrated over actions to obtain <img src="/summaries/tex/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode&sanitize=true" align=middle width=13.242037049999992pt height=22.465723500000017pt/>. In the continuous action case, ACER uses a Stochastic Dueling Network similar to Dueling DQN above, where one network estimates <img src="/summaries/tex/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode&sanitize=true" align=middle width=13.242037049999992pt height=22.465723500000017pt/> value, and another network estimates <img src="/summaries/tex/ea33e312aa38c6f22212fb5fb152a050.svg?invert_in_darkmode&sanitize=true" align=middle width=49.481371499999995pt height=30.267491100000004pt/> value which models <img src="/summaries/tex/704e45646d07e4a6f3e16a3efdb659ad.svg?invert_in_darkmode&sanitize=true" align=middle width=102.63888359999999pt height=24.65753399999998pt/> minus the average of a group of <img src="/summaries/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>'s sampled with distribution according to the policy NN. This <img src="/summaries/tex/7ce345688449b81ed1bca5342e17df89.svg?invert_in_darkmode&sanitize=true" align=middle width=12.99542474999999pt height=30.267491100000004pt/> provides a training target for <img src="/summaries/tex/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode&sanitize=true" align=middle width=13.242037049999992pt height=22.465723500000017pt/>. For trust region, the output probabilities are modeled by Gaussian distribution instead of discrete probability vector.

* [**Proximal Policy Optimization(PPO)**](https://arxiv.org/pdf/1707.06347.pdf): This is a variation of TRPO that is much simpler. Instead of using constraint of KL-divergence to stay in trusted region, the objective to maximize is clipped: <img src="/summaries/tex/421e7624df12f163f4d9cc6bae2e35a4.svg?invert_in_darkmode&sanitize=true" align=middle width=41.61562515pt height=33.20539859999999pt/> is clipped at <img src="/summaries/tex/190c0eb62bba4e6ac675100eb17a6db6.svg?invert_in_darkmode&sanitize=true" align=middle width=88.39013864999998pt height=24.65753399999998pt/>, and the lower one is kept to be pessimistic.

  The training uses the Actor-Critic style. The actor (policy) and critic (value) share NN parameters. <img src="/summaries/tex/f9c4988898e7f532b9f826a75014ed3c.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99998994999999pt height=22.465723500000017pt/> actors rollout for <img src="/summaries/tex/2f118ee06d05f3c2d98361d9c30e38ce.svg?invert_in_darkmode&sanitize=true" align=middle width=11.889314249999991pt height=22.465723500000017pt/> steps, and critic estimate the advantage function. The entire score to maximize by SGD has three parts: the clipped objective, the negative value function error, and entropy to encourage exploration.

  Note: This is an **on-policy algorithm**, although does not have to be in practice by [this](https://github.com/openai/baselines/issues/316).

* [**Evolution**](https://arxiv.org/abs/1703.03864): This one is special because there is no backpropagation. Starting with some parameters of the NN or any action model, a group of random parameters nearby are sampled, and the entire group is reweighted according to their score and averaged. So this is like an evolution procedure. It introduces randomness in the parameter space, instead of the action space. To make samples produce more distinct outcomes, virtual batch normalization is used, meaning a fixed reference batch is set aside to normalize each sample.

